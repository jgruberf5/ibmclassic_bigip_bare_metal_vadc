#cloud-config
package_update: true
package_upgrade: true
packages:
  - bridge-utils
  - cpu-checker
  - libvirt-clients
  - libvirt-daemon
  - libvirt-daemon-system
  - qemu
  - qemu-kvm
  - genisoimage
  - ipcalc
bootcmd:
  - [ cloud-init-per, once, mkdir, -m, 0755, -p, '/opt/F5Networks/onboarding' ]
write_files:
  - path: /opt/F5Networks/onboarding/installenv
    permissions: 0755
    content: |
      #!/bin/bash
      
      export script_dir=/opt/F5Networks/onboarding
      
      
      if [ -f "/etc/bigip" ]; then
          source /etc/bigip
      fi
      
      
      function set_permissions {
          chmod +x $script_dir/scripts/start
          chmod +x $script_dir/scripts/stop
      }
      
      
      function assure_packages {
        echo "Installing hypervisor and qemu agent requirements"
        apt update
        for pkg in bridge-utils cpu-checker libvirt-clients libvirt-daemon libvirt-daemon-system qemu qemu-kvm genisoimage; do
            installed=$(dpkg-query -W --showformat='${Status}\n' $pkg|grep "install ok installed"|wc -l)
            if [ "$installed" -ne "1" ]; then
                apt install -y $pkg
            fi 
        done
      }
      
      
      function create_netplan {
          echo "Creating BIG-IP virtual edition management access and activating SR-IOV virtual functions"
          
          NETPLAN_ACTIVE=/etc/netplan/01-netcfg.yaml
          NETPLAN_BACKUP=/opt/F5Networks/onboarding/netplan-orginial.yaml
          # original copy needed
          if ! [ -e $NETPLAN_BACKUP ]; then
            cp $NETPLAN_ACTIVE $NETPLAN_BACKUP
          fi
          
          # Read IP from bond0 (private) and populate BIG-IP netplan YAML to move IP address to bridge with BIG-IP management port
          #
          # This method uses linux network settings - assumes networking is up
          #PRIVATE_INTERFACE=bond0
          #PRIVATE_IP_ADDRESS=$(ip addr show $PRIVATE_INTERFACE | grep "inet " | awk '{print $2}' | cut -d'/' -f1)
          #PRIVATE_IP_MASK=$(ip addr show $PRIVATE_INTERFACE | grep "inet " | awk '{print $2}' | cut -d'/' -f2)
          #PRIVATE_NEXT_HOP=$(ip route show 10.0.0.0/8 | awk '{print $3}')
          #
          # This method uses the existing Ubuntu netplan YAML for IBM Classic bare metal servers
          # If the default netplan changes, this will need to be adjusted
          #
          # private ip addres = networks.bonds.bond0.address.0
          # private ip next hop = networks.bonds.bond0.routes.0.via
          #
          PRIVATE_IP_ADDRESS=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["addresses"][0])'| cut -d'/' -f1)
          PRIVATE_IP_MASK=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["addresses"][0])'| cut -d'/' -f2)
          PRIVATE_NEXT_HOP=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["routes"][0]["via"])')
      
          # Read IP from bond1 (public) and populate BIG-IP netplan YAML
          #
          # This method uses linux network settings - assumes networking is up
          #PUBLIC_INTERFACE=bond1
          #PUBLIC_IP_ADDRESS=$(ip addr show $PUBLIC_INTERFACE | grep "inet " | awk '{print $2}' | cut -d'/' -f1)
          #PUBLIC_IP_MASK=$(ip addr show $PUBLIC_INTERFACE | grep "inet " | awk '{print $2}' | cut -d'/' -f2)
          #PUBLIC_NEXT_HOP=$(ip route show default | awk '{print $3}')
          #
          # This method uses the existing Ubuntu netplan YAML for IBM Classic bare metal servers
          # If the default netplan changes, this will need to be adjusted
          #
          # public ip addres = networks.bonds.bond1.address.0
          # public ip next hop = networks.bonds.bond1.gateway4
          #
          PUBLIC_IP_ADDRESS=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond1"]["addresses"][0])' | cut -d'/' -f1)
          PUBLIC_IP_MASK=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond1"]["addresses"][0])' | cut -d'/' -f2)
          PUBLIC_NEXT_HOP=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond1"]["gateway4"])')
          #
          # Read DNS server for private DNS servers
          #
          # This method uses what is statically set in the linux resolver config - assumes netplan has created the config
          #DNS_1=$(cat /etc/resolv.conf | grep -m 1 nameserver | awk '{ print $2 }')
          #DNS_2=$(cat /etc/resolv.conf | grep -m 2 nameserver | tail -n1 | awk '{ print $2 }')
          #
          # This method uses the existing Ubuntu netplan YAML for IBM Classic bare metal servers
          # If the default netplan changes, this will need to be adjusted
          #
          export DNS_1=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["nameservers"]["addresses"][0])')
          export DNS_2=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["nameservers"]["addresses"][1])')
          #
          NETPLAN_TEMPLATE="$script_dir/netplan_template.yaml"
          NETPLAN_OUTPUT="$script_dir/netplan.yaml"
      
          third=$(cat /sys/class/net/bond0/address | cut -d':' -f5)
          third=$((16#$third))
          fourth=$(cat /sys/class/net/bond0/address | cut -d':' -f6)
          fourth=$((16#$fourth))
          HOST_LINK_LOCAL_MANAGEMENT_IP="169.254.$third.$fourth/16"
      
          cp $NETPLAN_TEMPLATE $NETPLAN_OUTPUT
          sed -i "s|__DNS_1__|$DNS_1|g" $NETPLAN_OUTPUT
          sed -i "s|__DNS_2__|$DNS_2|g" $NETPLAN_OUTPUT
          sed -i "s|__PUBLIC_IP_ADDRESS__|$PUBLIC_IP_ADDRESS|g" $NETPLAN_OUTPUT
          sed -i "s|__PUBLIC_IP_MASK__|$PUBLIC_IP_MASK|g" $NETPLAN_OUTPUT
          sed -i "s|__PUBLIC_NEXT_HOP__|$PUBLIC_NEXT_HOP|g" $NETPLAN_OUTPUT
          sed -i "s|__PRIVATE_IP_ADDRESS__|$PRIVATE_IP_ADDRESS|g" $NETPLAN_OUTPUT
          sed -i "s|__PRIVATE_IP_MASK__|$PRIVATE_IP_MASK|g" $NETPLAN_OUTPUT
          sed -i "s|__PRIVATE_NEXT_HOP__|$PRIVATE_NEXT_HOP|g" $NETPLAN_OUTPUT
          sed -i "s|__HOST_LINK_LOCAL_MANAGEMENT_IP__|$HOST_LINK_LOCAL_MANAGEMENT_IP|g" $NETPLAN_OUTPUT
          # 
          # install netplan and activacte
          #
          echo "Applying Netplan"
          cp $NETPLAN_OUTPUT $NETPLAN_ACTIVE
          netplan apply
      }
      
      
      function create_bigip_userdata {
          echo "Creating BIG-IP cloud-init user_data"
          BIGIP_CLOUDINIT_USERDATA_TEMPLATE="$script_dir/BIGIPUserDataTemplate.yaml"
          BIGIP_CLOUD_USERDATA="$script_dir/BIGIPUserData.yaml"
          cp $BIGIP_CLOUDINIT_USERDATA_TEMPLATE $BIGIP_CLOUD_USERDATA
          NETPLAN_ACTIVE=/etc/netplan/01-netcfg.yaml
          NETPLAN_BACKUP=/opt/F5Networks/onboarding/netplan-orginial.yaml
          # original copy needed
          if ! [ -e $NETPLAN_BACKUP ]; then
            cp $NETPLAN_ACTIVE $NETPLAN_BACKUP
          fi
          #
          # Read SSH key added to bare metal server and add to BIG-IP virtual edition
          #
          SSH_AUTH_KEY=$(cat /root/.ssh/authorized_keys | grep -Ev "^#|^$")
          #
          # Build cloudinit CI data ISO disk
          # 
          sed -i "s|__BIGIP_SSH_AUTH_KEY__|$BIGIP_SSH_AUTH_KEY|g" $BIGIP_CLOUD_USERDATA
          [[ -z "${TMOS_ADMIN_PASSWORD}" ]] && TMOS_ADMIN_PASSWORD='F5Networks!'
          sed -i "s|__TMOS_ADMIN_PASSWORD__|$TMOS_ADMIN_PASSWORD|g" $BIGIP_CLOUD_USERDATA
          [[ -z "${BIGIP_HOSTNAME}" ]] && BIGIP_HOSTNAME=$(hostname)
          sed -i "s|__BIGIP_HOSTNAME__|$BIGIP_HOSTNAME|g" $BIGIP_CLOUD_USERDATA
          DNS_1=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["nameservers"]["addresses"][0])')
          DNS_2=$(cat $NETPLAN_BACKUP | python3 -c 'import json, sys, yaml ; y = yaml.safe_load(sys.stdin.read()) ; print(y["network"]["bonds"]["bond0"]["nameservers"]["addresses"][1])')    
          sed -i "s|__DNS_1__|$DNS_1|g" $BIGIP_CLOUD_USERDATA
          sed -i "s|__DNS_2__|$DNS_2|g" $BIGIP_CLOUD_USERDATA
          third=$(cat /sys/class/net/bond0/address | cut -d':' -f5)
          third=$((16#$third))
          fourth=$(cat /sys/class/net/bond0/address | cut -d':' -f6)
          fourth=$((16#$fourth))
          HOST_LINK_LOCAL_MANAGEMENT_IP="169.254.$third.$fourth"
          fourth_next=$(($fourth + 1))
          [[ -z "${BIGIP_MANAGEMENT_IP}" ]] && BIGIP_MANAGEMENT_IP="169.254.$third.$fourth_next/16"
          sed -i "s|__BIGIP_MANAGEMENT_IP__|$BIGIP_MANAGEMENT_IP|g" $BIGIP_CLOUD_USERDATA
          [[ -z "${BIGIP_MANAGEMENT_NEXT_HOP}" ]] && BIGIP_MANAGEMENT_NEXT_HOP=$HOST_LINK_LOCAL_MANAGEMENT_IP
          sed -i "s|__BIGIP_MANAGEMENT_NEXT_HOP__|$BIGIP_MANAGEMENT_NEXT_HOP|g" $BIGIP_CLOUD_USERDATA
          [[ -z "${BIGIP_MANAGEMENT_MTU}" ]] && BIGIP_MANAGEMENT_MTU="1460"
          sed -i "s|__BIGIP_MANAGEMENT_MTU__|$BIGIP_MANAGEMENT_MTU|g" $BIGIP_CLOUD_USERDATA
          [[ -z "${BIGIP_INSTANCE_ID}" ]] && BIGIP_INSTANCE_ID="bigipinstance1"
          rm -rf "$script_dir/cidataiso"
          mkdir -p "$script_dir/cidataiso"
          echo "instance-id: {{ $BIGIPecho "Type=oneshot" >> $UNIT_FILE_INSTANCE_ID }}" >> "$script_dir/cidataiso/meta-data"
          echo "local-hostname: {{ $BIGIP_HOSTNAME }}" >> "$script_dir/cidataiso/meta-data"
          mv $BIGIP_CLOUD_USERDATA "$script_dir/cidataiso/user-data"
          pushd $script_dir/cidataiso
          mkisofs -V cidata -lJR -o output.iso meta-data user-data
          popd
          cp $script_dir/cidataiso/output.iso /opt/F5Networks/onboarding/BIGIP_USER_DATA.iso
      }
      
      
      function create_bigip_domain_xml {
          [[ -z "${BIGIP_VM_NAME}" ]] && BIGIP_VM_NAME="BIG-IP-Virtual-Edition"
          defined=$(virsh list --all | grep $BIGIP_VM_NAME | wc -l)
          if [ "$defined" -ne "1" ]; then    
              echo "Creating libvirt domain XML for BIG-IP virtual edition"
              BIGIP_DOMAIN_TEMPLATE="$script_dir/BIGIPDomainTemplate.xml"
              BIGIP_DOMAIN="$script_dir/BIGIPDomain.xml"
              BIGIP_IMAGE_FILE="$script_dir/BIGIPImages/BIG-IP-Instance-Disk.qcow2"
              BIGIP_CLOUDINIT_ISO=/opt/F5Networks/onboarding/BIGIP_USER_DATA.iso
              [[ -z "${BIGIP_VM_MEMORY}" ]] && BIGIP_VM_MEMORY="4194304"  
              [[ -z "${BIGIP_VM_VCPUS}" ]] && BIGIP_VM_VCPUS="2"   
              cp $BIGIP_DOMAIN_TEMPLATE $BIGIP_DOMAIN
              sed -i 's|__BIGIP_VM_NAME__|'${BIGIP_VM_NAME}'|g' $BIGIP_DOMAIN
              sed -i "s|__BIGIP_VM_MEMORY__|$BIGIP_VM_MEMORY|g" $BIGIP_DOMAIN
              sed -i "s|__BIGIP_VM_VCPUS__|$BIGIP_VM_VCPUS|g" $BIGIP_DOMAIN
              sed -i 's|__BIGIP_IMAGE_FILE__|'${BIGIP_IMAGE_FILE}'|g' $BIGIP_DOMAIN
              sed -i 's|__BIGIP_CLOUDINIT_ISO__|'${BIGIP_CLOUDINIT_ISO}'|g' $BIGIP_DOMAIN
              virsh define $BIGIP_DOMAIN
          fi
      }
      
      
      function download_bigip_image {
        [[ -z "${BIGIP_IMAGE_DOWNLOAD_PATH}" ]] && BIGIP_IMAGE_DOWNLOAD_PATH='https://s3.us-east.cloud-object-storage.appdomain.cloud/f5-adc-bigip-17.5.0-0.0.15.all-1slot-031025001-us-east'
        [[ -z "${BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME}" ]] && BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME='BIGIP-17.5.0-0.0.15.ALL_1SLOT-031025001.qcow2'
        [[ -z "${BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5}" ]] && BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5="$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME.md5"
        if [ -f "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME" ] && [ -f "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5" ] && [ -f "$script_dir/BIGIPImages/BIG-IP-Instance-Disk.qcow2" ]; then
            echo "Existing BIG-IP virtual edition disk exists.. skipping download"
        else
          mkdir -p "$script_dir/BIGIPImages"
          image_url="$BIGIP_IMAGE_DOWNLOAD_PATH/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME"
          md5_url="$BIGIP_IMAGE_DOWNLOAD_PATH/$BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5"
          # Download with MD5 check
          download_md5=0
          image_md5=1
          while [ ! "$image_md5" == "$download_md5" ]; do
            rm -rf "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME"
            echo "Downloading $BIGIP_IMAGE_DOWNLOAD_PATH/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME..."
            wget --retry-connrefused --waitretry=1 --read-timeout=30 --timeout=15 -t 0 -nc $BIGIP_IMAGE_DOWNLOAD_PATH/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME -P "$script_dir/BIGIPImages"
            rm -rf "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5"
            echo "Downloading $BIGIP_IMAGE_DOWNLOAD_MD5/$BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5..."
            wget --retry-connrefused --waitretry=1 --read-timeout=30 --timeout=15 -t 0 -nc $BIGIP_IMAGE_DOWNLOAD_PATH/$BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5 -P "$script_dir/BIGIPImages"
            download_md5=$(md5sum "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME" | cut -d' ' -f1)
            image_md5=$(cat "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_MD5")
          done
          cp "$script_dir/BIGIPImages/$BIGIP_IMAGE_DOWNLOAD_IMAGE_NAME" "$script_dir/BIGIPImages/BIG-IP-Instance-Disk.qcow2"
        fi
      }
      
      
      function add_port_forward_management_traffic {
          [[ -z "${MANAGEMENT_PORT_FORWARDS}" ]] && MANAGEMENT_PORT_FORWARDS=1
          if [ "$MANAGEMENT_PORT_FORWARDS" -eq "1" ]; then
              third=$(cat /sys/class/net/bond0/address | cut -d':' -f5)
              third=$((16#$third))
              fourth=$(cat /sys/class/net/bond0/address | cut -d':' -f6)
              fourth=$((16#$fourth))
              fourth_next=$(($fourth + 1))
              [[ -z "${BIGIP_MANAGEMENT_IP}" ]] && BIGIP_MANAGEMENT_IP="169.254.$third.$fourth_next"
              if [[ "$BIGIP_MANAGEMENT_IP" == */* ]]; then
                  BIGIP_MANAGEMENT_IP=$(echo $BIGIP_MANAGEMENT_IP|cut -d'/' -f1)
              fi
              # managment bridge forwards
              iptables -t nat -i br0 -A PREROUTING -p tcp --dport 443 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:443
              iptables -A FORWARD -i br0 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 443 -j ACCEPT
              iptables -t nat -i br0 -A PREROUTING -p tcp --dport 2222 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:22
              iptables -A FORWARD -i br0 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 22 -j ACCEPT
              iptables -t nat -i br0 -A POSTROUTING -j MASQUERADE
              # management bridge 
              iptables -t nat -i bond1 -A PREROUTING -p tcp --dport 443 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:443
              iptables -A FORWARD -i bond1 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 443 -j ACCEPT
              iptables -t nat -i bond1 -A PREROUTING -p tcp --dport 2222 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:22
              iptables -A FORWARD -i bond1 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 22 -j ACCEPT
              iptables -t nat -i bond1 -A POSTROUTING -j MASQUERADE
      
          fi
      }
      
      
      function remove_port_forward_management_traffic {
          [[ -z "${MANAGEMENT_PORT_FORWARDS}" ]] && MANAGEMENT_PORT_FORWARDS=1
          if [ "$MANAGEMENT_PORT_FORWARDS" -eq "1" ]; then
              third=$(cat /sys/class/net/bond0/address | cut -d':' -f5)
              third=$((16#$third))
              fourth=$(cat /sys/class/net/bond0/address | cut -d':' -f6)
              fourth=$((16#$fourth))
              fourth_next=$(($fourth + 1))
              [[ -z "${BIGIP_MANAGEMENT_IP}" ]] && BIGIP_MANAGEMENT_IP="169.254.$third.$fourth_next"
              if [[ "$BIGIP_MANAGEMENT_IP" == */* ]]; then
                  BIGIP_MANAGEMENT_IP=$(echo $BIGIP_MANAGEMENT_IP|cut -d'/' -f1)
              fi
              # management bridge forwards
              iptables -t nat -i br0 -D PREROUTING -p tcp --dport 443 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:443
              iptables -D FORWARD -i br0 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 443 -j ACCEPT
              iptables -t nat -i br0 -D PREROUTING -p tcp --dport 2222 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:22
              iptables -D FORWARD -i br0 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 22 -j ACCEPT
              iptables -t nat -i br0 -D POSTROUTING -j MASQUERADE
              # management bridge forwards
              ptables -t nat -i bond1 -D PREROUTING -p tcp --dport 443 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:443
              iptables -D FORWARD -i bond1 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 443 -j ACCEPT
              iptables -t nat -i bond1 -D PREROUTING -p tcp --dport 2222 -j DNAT --to-destination $BIGIP_MANAGEMENT_IP:22
              iptables -D FORWARD -i bond1 -p tcp -d $BIGIP_MANAGEMENT_IP --dport 22 -j ACCEPT
              iptables -t nat -i bond1 -D POSTROUTING -j MASQUERADE
          fi
      }
      
      
      function start_vm {
          add_port_forward_management_traffic
          [[ -z "${BIGIP_VM_NAME}" ]] && BIGIP_VM_NAME="BIG-IP-Virtual-Edition"
          running=$(virsh list --all | grep $BIGIP_VM_NAME | grep 'running' | wc -l)
          shutoff=$(virsh list --all | grep $BIGIP_VM_NAME | grep 'shut off' | wc -l)
          if [ "$running" -eq "1" ]; then
              echo "$BIGIP_VM_NAME is already running.."
          else
              virsh start $BIGIP_VM_NAME
          fi
      }
      
      
      function shutdown_vm {
          remove_port_forward_management_traffic
          [[ -z "${BIGIP_VM_NAME}" ]] && BIGIP_VM_NAME="BIG-IP-Virtual-Edition"
          shutoff=$(virsh list --all | grep $BIGIP_VM_NAME | grep 'shut off' | wc -l)
          if [ "$shutoff" -eq "1" ]; then
              echo "$BIGIP_VM_NAME is already shutdown.."
          else
              virsh shutdown $BIGIP_VM_NAME
          fi
      }
      
      
      function create_systemd_unit_file {
          UNIT_FILE="/etc/systemd/system/bigip-virtual-edition.service"
          echo "[Unit]" > $UNIT_FILE
          echo "Description=BIG-IP virtual edition service" >> $UNIT_FILE
          echo "After=network.target" >> $UNIT_FILE
          echo "StartLimitIntervalSec=0" >> $UNIT_FILE
          echo "" >> $UNIT_FILE
          echo "[Service]" >> $UNIT_FILE
          echo "WorkingDirectory=/opt/F5Networks/onboarding" >> $UNIT_FILE
          echo "Type=oneshot" >> $UNIT_FILE
          echo "User=root" >> $UNIT_FILE
          echo "ExecStart=/opt/F5Networks/onboarding/start" >> $UNIT_FILE
          echo "ExecStop=/opt/F5Networks/onboarding/stop" >> $UNIT_FILE
          echo "RemainAfterExit=1" >> $UNIT_FILE
          echo "" >> $UNIT_FILE
          echo "[Install]" >> $UNIT_FILE
          echo "WantedBy=multi-user.target" >> $UNIT_FILE
      }
      
      
      function enable_systemd_service {
          systemctl enable bigip-virtual-edition.service
      }
      
      
      function disable_systemd_service {
          systemctl disable bigip-virtual-edition.service
      }
      
      
      function start_systemd_service {
          systemctl start bigip-virtual-edition.service
      }
      
      
      function stop_systemd_service {
          systemctl stop bigip-virtual-edition.service
      }
      
      
      function start {
        assure_packages 
        download_bigip_image
        create_netplan
        create_bigip_userdata
        create_bigip_domain_xml
        start_vm
      }
      
      
      function stop {
        shutdown_vm
  - path: /opt/F5Networks/onboarding/onboarding.sh
    permissions: 0755
    content: |
      #!/bin/bash
      
      export script_dir=/opt/F5Networks/onboarding
      
      # Source the installenv file
      if [ -f "$script_dir/installenv" ]; then
        source "$script_dir/installenv"
      else
        echo "Error: installenv file not found in $script_dir"
        exit 1
      fi
      
      set_permissions
      create_systemd_unit_file
      enable_systemd_service
  - path: /opt/F5Networks/onboarding/start
    permissions: 0755
    content: |
      #!/bin/bash
      
      export script_dir=/opt/F5Networks/onboarding
      
      # Source the installenv file
      if [ -f "$script_dir/installenv" ]; then
        source "$script_dir/installenv"
      else
        echo "Error: installenv file not found in $script_dir"
        exit 1
      fi
      
  - path: /opt/F5Networks/onboarding/stop
    permissions: 0755
    content: |
      #!/bin/bash
      
      export script_dir=/opt/F5Networks/onboarding
      
      # Source the installenv file
      if [ -f "$script_dir/installenv" ]; then
        source "$script_dir/installenv"
      else
        echo "Error: installenv file not found in $script_dir"
        exit 1
      fi
      
  - path: /opt/F5Networks/onboarding/BIGIPDomainTemplate.xml
    permissions: 0644
    content: |
      <domain type="kvm">
          <name>__BIGIP_VM_NAME__</name>
          <metadata>
            <libosinfo:libosinfo xmlns:libosinfo="http://libosinfo.org/xmlns/libvirt/domain/1.0">
              <libosinfo:os id="http://libosinfo.org/linux/2022"/>
            </libosinfo:libosinfo>
          </metadata>
          <memory>__BIGIP_VM_MEMORY__</memory>
          <currentMemory>__BIGIP_VM_MEMORY__</currentMemory>
          <vcpu>__BIGIP_VM_VCPUS__</vcpu>
          <os>
            <type arch="x86_64" machine="q35">hvm</type>
            <boot dev="hd"/>
          </os>
          <features>
            <acpi/>
            <apic/>
            <vmport state="off"/>
          </features>
          <cpu mode="host-model"/>
          <clock offset="utc">
            <timer name="rtc" tickpolicy="catchup"/>
            <timer name="pit" tickpolicy="delay"/>
            <timer name="hpet" present="no"/>
          </clock>
          <pm>
            <suspend-to-mem enabled="no"/>
            <suspend-to-disk enabled="no"/>
          </pm>
          <devices>
            <emulator>/usr/bin/qemu-system-x86_64</emulator>
            <disk type="file" device="disk">
              <driver name="qemu" type="qcow2"/>
              <source file="__BIGIP_IMAGE_FILE__"/>
              <target dev="vda" bus="virtio"/>
            </disk>
            <disk type="file" device="cdrom">
              <driver name="qemu" type="raw"/>
              <source file="__BIGIP_CLOUDINIT_ISO__"/>
              <target dev="sda" bus="sata"/>
              <readonly/>
            </disk>
            <controller type="usb" model="qemu-xhci" ports="15"/>
            <controller type="pci" model="pcie-root"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <controller type="pci" model="pcie-root-port"/>
            <interface type="bridge">
              <source bridge="br0"/>
              <model type="virtio"/>
            </interface>
            <!--
            Use: lshw -class network -businfo   
            to get the PCI addresses of the devices.
      
            Remember these are wired in order eth0vf0, then eth2vf0
      
            so the bridge interface you are replace for:
      
            eth0vf0 = ens1f0v0
            eth2vf0 = ens1f2v0
      
            Hardware addressing for IBM Bare Metal instances - PCI addresses are hardcoded and 
            not templated
      
            Assure the Intel SR-IOV NIC is in the same bus and slot for all deployments
      
            lshw -class network -businfo
            Bus info          Device           Class          Description
            =============================================================
            pci@0000:06:00.0  eth0             network        Ethernet Controller X710/X557-AT 10GBASE-T
            pci@0000:06:00.1  eth1             network        Ethernet Controller X710/X557-AT 10GBASE-T
            pci@0000:06:00.2  eth2             network        Ethernet Controller X710/X557-AT 10GBASE-T
            pci@0000:06:00.3  eth3             network        Ethernet Controller X710/X557-AT 10GBASE-T
            pci@0000:06:02.0  ens1f0v0         network        Ethernet Virtual Function 700 Series
            pci@0000:06:02.1  ens1f0v1         network        Ethernet Virtual Function 700 Series
            pci@0000:06:06.0  ens1f1v0         network        Ethernet Virtual Function 700 Series
            pci@0000:06:06.1  ens1f1v1         network        Ethernet Virtual Function 700 Series
            pci@0000:06:0a.0  ens1f2v0         network        Ethernet Virtual Function 700 Series
            pci@0000:06:0a.1  ens1f2v1         network        Ethernet Virtual Function 700 Series
            pci@0000:06:0e.0  ens1f3v0         network        Ethernet Virtual Function 700 Series
            pci@0000:06:0e.1  ens1f3v1         network        Ethernet Virtual Function 700 Series
      
            -->
            <!-- PCI Passthrough for VF ens1f0v0  pci@0000:06:02.0 -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x02' function='0x0'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth0vf0"/>
              <model type="virtio"/>
            </interface>
            <!-- PCI Passthrough for VF pci@0000:06:0a.0  ens1f2v0  -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x0a' function='0x0'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth2vf0"/>
              <model type="virtio"/>
            </interface>
            <!-- PCI Passthrough for VF pci@0000:06:06.0 ens1f1v0 -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x06' function='0x0'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
            </interface>
            -->
            <!--bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth1vf0"/>
              <model type="virtio"/>
            </interface>
            <!-- PCI Passthrough for VF pci@0000:06:0e.0  ens1f3v0   -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x0e' function='0x0'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth3vf0"/>
              <model type="virtio"/>
            </interface>
            <!-- PCI Passthrough for VF ppci@0000:06:02.1  ens1f0v1   -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x02' function='0x1'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth0vf1"/>
              <model type="virtio"/>
            </interface>
            <!--  PCI Passthrough for VF pci@0000:06:0a.1  ens1f2v1   -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x0a' function='0x1'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth2vf1"/>
              <model type="virtio"/>
            </interface>
            <!--  PCI Passthrough for VF pci@0000:06:06.1  ens1f1v1   -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x06' function='0x1'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x0d' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth1vf1"/>
              <model type="virtio"/>
            </interface>
            <!-- PCI Passthrough for VF pci@0000:06:0e.1  ens1f3v1   -->
            <!--
            <interface type='hostdev' managed='yes'>
            <source>
                <address type='pci' domain='0x0000' bus='0x6' slot='0x0e' function='0x1'/>
            </source>
            <address type='pci' domain='0x0000' bus='0x00' slot='0x0e' function='0x0'/>
            </interface>
            -->
            <!-- bridge implementation for test -->
            <interface type="bridge">
              <source bridge="eth3vf1"/>
              <model type="virtio"/>
            </interface>
            <console type="pty"/>
            <channel type="unix">
              <source mode="bind"/>
              <target type="virtio" name="org.qemu.guest_agent.0"/>
            </channel>
            <channel type="spicevmc">
              <target type="virtio" name="com.redhat.spice.0"/>
            </channel>
            <input type="tablet" bus="usb"/>
            <graphics type="spice" port="-1" tlsPort="-1" autoport="yes"/>
            <sound model="ich9"/>
            <video>
              <model type="virtio"/>
            </video>
            <redirdev bus="usb" type="spicevmc"/>
            <redirdev bus="usb" type="spicevmc"/>
            <memballoon model="virtio"/>
            <rng model="virtio">
              <backend model="random">/dev/urandom</backend>
            </rng>
          </devices>
      </domain>
  - path: /opt/F5Networks/onboarding/netplan_template.yaml
    permissions: 0644
    content: |
      network:
        version: 2
        renderer: networkd
        ethernets:
          eth0:
            dhcp4: no
            dhcp6: no
            nameservers:
              addresses:
                - __DNS_1__
                - __DNS_2__
            virtual-function-count: 2
          eth1:
            dhcp4: no
            dhcp6: no
            nameservers:
              addresses:
                - __DNS_1__
                - __DNS_2__
            virtual-function-count: 2
          eth2:
            dhcp4: no
            dhcp6: no
            nameservers:
              addresses:
                - __DNS_1__
                - __DNS_2__
            virtual-function-count: 2
          eth3:
            dhcp4: no
            dhcp6: no
            nameservers:
              addresses:
                - __DNS_1__
                - __DNS_2__
            virtual-function-count: 2
          vf1:
            match:
              name: ens1f0v0
            link: eth0
            dhcp4: no
            dhcp6: no
          vf2:
            match:
              name: ens1f0v1
            link: eth0
            dhcp4: no
            dhcp6: no
          vf3:
            match:
              name: ens1f1v0
            link: eth1
            dhcp4: no
            dhcp6: no
          vf4:
            match:
              name: ens1f1v1
            link: eth1
            dhcp4: no
            dhcp6: no
          vf5:
            match:
              name: ens1f2v0
            link: eth2
            dhcp4: no
            dhcp6: no
          vf6:
            match:
              name: ens1f2v1
            link: eth2
            dhcp4: no
            dhcp6: no
          vf7:
            match:
              name: ens1f3v0
            link: eth3
            dhcp4: no
            dhcp6: no
          vf8:
            match:
              name: ens1f3v1
            link: eth3
            dhcp4: no
            dhcp6: no
        bonds:
          bond0:
            interfaces:
              - eth0
              - eth2
            parameters:
              mode: 802.3ad
              lacp-rate: fast
              mii-monitor-interval: 100
              transmit-hash-policy: layer3+4
              all-slaves-active: true
              up-delay: 0
              down-delay: 0
          bond1:
            interfaces:
              - eth1
              - eth3
            addresses: [__PUBLIC_IP_ADDRESS__/__PUBLIC_IP_MASK__]
            gateway4: __PUBLIC_NEXT_HOP__
            nameservers:
              addresses:
                - __DNS_1__
                - __DNS_2__
            parameters:
              mode: 802.3ad
              lacp-rate: fast
              mii-monitor-interval: 100
              transmit-hash-policy: layer3+4
              all-slaves-active: true
              up-delay: 0
              down-delay: 0
        bridges:
          br0:
            addresses: [__PRIVATE_IP_ADDRESS__/__PRIVATE_IP_MASK__, __HOST_LINK_LOCAL_MANAGEMENT_IP__]
            interfaces: [bond0]
            nameservers:
              addresses:
                - __DNS_1__
                - __DNS_2__
            routes:
              - to: 10.0.0.0/8
                via: __PRIVATE_NEXT_HOP__
              - to: 161.26.0.0/16
                via: __PRIVATE_NEXT_HOP__
              - to: 166.8.0.0/14
                via: __PRIVATE_NEXT_HOP__
      #
      #  Bridge implementation for BIG-IP virtual addition testing
      #
      #  Changes here will require changes to the libvirt domain XML to match
      #
          eth0vf0:
            interfaces: [vf1]
          eth0vf1:
            interfaces: [vf2]
          eth1vf0:
            interfaces: [vf3]
          eth1vf1:
            interfaces: [vf4]
          eth2vf0:
            interfaces: [vf5]
          eth2vf1:
            interfaces: [vf6]
          eth3vf0:
            interfaces: [vf7]
          eth3vf1:
            interfaces: [vf8]
  - path: /opt/F5Networks/onboarding/BIGIPUserDataTemplate.yaml
    permissions: 0644
    content: |
      #cloud-config
      chpasswd:
        list: |
          root:__TMOS_ADMIN_PASSWORD__
          admin:__TMOS_ADMIN_PASSWORD__
        expire: False
      ssh_authorized_keys:
        - ssh-rsa [ __BIGIP_SSH_AUTH_KEY__ ]
      tmos_static_mgmt:
        enabled: true
        icontrollx_trusted_sources: false
        ip: __BIGIP_MANAGEMENT_IP__
        gw: __BIGIP_MANAGEMENT_NEXT_HOP__
        mtu: __BIGIP_MANAGEMENT_MTU__
        post_onboard_enabled: true
        post_onboard_commands:
          - tmsh create net trunk slPrivate interfaces replace-all-with { 1.1 1.2 }
          - tmsh create net trunk slPublic interfaces replace-all-with { 1.3 1.4 }
          - tmsh create net trunk customerPrivate interfaces replace-all-with { 1.5 1.6 }
          - tmsh create net trunk customerPublic interfaces replace-all-with { 1.7 1.8 }
      tmos_declared:
        enabled: true
        icontrollx_trusted_sources: false
        do_declaration:
          schemaVersion: 1.0.0
          class: Device
          async: true
          label: Cloudinit Onboarding
          Common:
            class: Tenant
            bigipSystem:
              class: System
              hostname: __BIGIP_HOSTNAME__
              autoPhonehome: false
            provisioningLevels:
              class: Provision
              ltm: nominal
            dnsServers:
              class: DNS
              nameServers:
                - __DNS_1__
                - __DNS_2__
            slPrivate:
              class: Trunk
              interfaces:
                - "1.1"
                - "1.2"
            slPublic:
              class: Trunk
              interfaces:
                - "1.3"
                - "1.4"
            customerPrivate:
              class: Trunk
              interfaces:
                - "1.5"
                - "1.6"
            customerPublic:
              class: Trunk
              interfaces:
                - "1.7"
                - "1.8"
runcmd: [nohup sh -c '/opt/F5Networks/onboarding/onboarding.sh' >> /var/log/F5NetworksBIGIPOnboard.log &]
